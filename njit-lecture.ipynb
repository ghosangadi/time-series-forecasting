{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES FORECASTING \n",
    "\n",
    "**by Gurudutt Hosangadi**, \n",
    "\n",
    "**NJIT November 7, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1.  <a href='#intro' > Introduction </a>\n",
    "    1. <a href='#forecasting' > What is forecasting? </a>\n",
    "    2. <a href='#time-series' > What is a time series? </a>\n",
    "    3. <a href='#software' > Software </a>\n",
    "    4. <a href='#numpy' > Numpy Overview </a>\n",
    "    5. <a href='#pandas' > Pandas Overview </a>\n",
    "2. <a href='#time-series-analysis' > Time Series Analysis </a>\n",
    "    1.  <a href='#code1' > Coding Example: Read in a time series using Pandas </a>\n",
    "    2.  <a href='#patterns' > Patterns </a>\n",
    "    3.  <a href='#decompose' > Decomposition of Time Series </a>\n",
    "        1.  <a href='#ets' > ETS Model </a>\n",
    "        2.  <a href='#code2' > Coding Example: Decompostion of Time Series </a>\n",
    "    4. <a href='#endo-exo' > Endogenous versus Exogenous data </a>\n",
    "        1. <a href='#code3' > Coding Example: identifying endogenous and exogenous data </a>\n",
    "    5. <a href='#model-fit' > Model Fitting </a>      \n",
    "        1. <a href='#sma' > Simple Moving Average </a>\n",
    "        2. <a href='#ewma' > Exponentially Weighted Moving Average </a>\n",
    "            1. <a href='#time-constant' > Time constant </a>\n",
    "        3. <a href='#code4' > Coding Example: Model fitting of airline passenger data </a>\n",
    "3. <a href='#time-series-forecasting' > Time Series Forecasting </a>\n",
    "    1. <a href='#average-method' > Average Method </a>\n",
    "    2. <a href='#naive-method' > Naive Method </a>\n",
    "    3. <a href='#seasonal-naive-method' > Seasonal Naive Method </a>\n",
    "    4. <a href='#ex1' > Example forecast for Average/Naive/Seasonal Methods </a>\n",
    "    5. <a href='#impl-steps' > Implementation steps for forecasting </a>\n",
    "    6. <a href='#exp-smooth' > Simple Exponential Smoothing Method </a>\n",
    "    7. <a href='#holt-winters' > Holt Winters Method </a>\n",
    "         1. <a href='#double-exp-smooth' > Double Exponential Smoothing Method </a>\n",
    "         2. <a href='#triple-exp-smooth' > Triple Exponential Smoothing Method </a>\n",
    "         3. <a href='#code5' > Coding Example: Forecasting using Holt Winters Method </a>\n",
    "    2. <a href='#arima' > ARIMA Models\n",
    "        1. <a href='regress' > Regression </a>\n",
    "        2. <a href='lag' > Backshifting or Lagging </a>\n",
    "        3. <a href='stat' > Stationarity </a>\n",
    "        4. <a href='diff' > Differencing </a>\n",
    "        5. <a href='ex2' > Example of classifying time series as stationary versus non stationary </a>\n",
    "        6. <a href='arima-comp' > Components of ARIMA </a>\n",
    "        7. <a href='ar' > AR Models </a>\n",
    "        8. <a href='arma' > ARMA Models </a>\n",
    "            1. <a href='code6' > Coding Example: Forecasting using ARMA </a>\n",
    "4. <a href='misc'> MISC </a>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h3>Online Reference:</h3>\n",
    "<strong>\n",
    "<a href='https://otexts.com/fpp2/'>Forecasting: Principles and Practice</a></strong>&nbsp;&nbsp;<font color=black>by Rob J Hyndman and George Athanasopoulos</font><br>\n",
    "<strong>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "# Introduction\n",
    "<a id='forecasting'></a>\n",
    "## Forecasting\n",
    "* Forecasting is the process of predicting the future based on past and present data.\n",
    "* Forecasting is not new and has been prevalent throughout history.\n",
    "* Good forecasts:\n",
    "    * By analyzing population growth in Europe, Ezra Stiles, then president of Yale University, predicted in 1783 that America's population would reach 300 million in 200 years. A little over 200 years later, the U.S. population hit 300 million\n",
    "    \n",
    "    ![title](images/pop-us.png)\n",
    "    * \"It will soon be possible to transmit wireless messages all over the world so simply that any individual can own and operate his own apparatus,\" Nikola Tesla told The New York Times in 1909.\n",
    "    \n",
    "![title](images/wireless_pred.png)\n",
    "          \n",
    "* Poor forecasts:\n",
    "    * 1800: Rail travel at high speed is not possible, because passengers unable to breathe, would die of asphyxia. \n",
    "    * In 1943 the president of IBM predicted a world market \"for maybe five computers\".\n",
    "* Backbone of good forecasting is \"data\"\n",
    "    * In the case of population prediction, recorded population of Europe was used\n",
    "    * In the case of wireless prediction, theoretical data was used.\n",
    "    \n",
    "\n",
    "* Ability to forecast depends on several factors including:\n",
    "    * how well we understand the factors that contribute to it\n",
    "    * how much data is available\n",
    "    * whether the forecasts can affect the thing we are trying to forecast.\n",
    "\n",
    "\n",
    "### Steps in Forecasting\n",
    "![title](images/forecast-steps.png)\n",
    "\n",
    "\n",
    "### Qualitative forecasting versus quantitative forecasting\n",
    "\n",
    "If there are no data available, or if the data available are not relevant to the forecasts, then **qualitative forecasting** methods are generally used.\n",
    "\n",
    "**Quantitative forecasting** can be used when two conditions are satisfied:\n",
    "\n",
    "   * numerical information about the past is available;\n",
    "   * it is reasonable to assume that some aspects of the past patterns will continue into the future.\n",
    "\n",
    "\n",
    "Focus is on quantitative forecasting\n",
    "\n",
    "Most quantitative prediction problems use either time series data ( i.e. data collected over regular intervals in time) or cross-sectional data (collected at a single point in time). \n",
    "\n",
    "**In this lecture we will concentrate on the time series data.**\n",
    "\n",
    "<a id='time-series'></a>\n",
    "## Time Series\n",
    "\n",
    "A time series is a series of data points indexed in time order. Some examples are shown below:\n",
    "\n",
    "![title](images/time-series-examples.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this lecture\n",
    "\n",
    "By end of this lecture you will learn how to identify time series data, analyze them, fit models to the data and forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='software'></a>\n",
    "## Software that we will be using\n",
    "All examples will be based on python and we will be using the following libraries extensively:\n",
    "* Pandas\n",
    "* Numpy\n",
    "* Matplotlib\n",
    "* Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='numpy'></a>\n",
    "## Brief overview Numpy\n",
    "\n",
    "\n",
    "NumPy is a powerful linear algebra library in python and is one of the main building blocks of other libraries in python such as pandas, scikit-learn etc.\n",
    "\n",
    "NumPy arrays can be viewed as 1-dimensional vectors or 2-dimensional matrices (note that a matrix can still have only one row or one column).\n",
    "\n",
    "Let's begin our introduction by exploring how to create NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating NumPy Arrays\n",
    "\n",
    "Numpy arrays can be created from python list conversion or from in built methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first you need to import the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# here is a python list\n",
    "py_list = [3,4,5]\n",
    "\n",
    "# convert list to python array\n",
    "np_vec = np.array(py_list)\n",
    "print(\"np_vec=\\n\",np_vec)\n",
    "\n",
    "# here is a python list of lists\n",
    "py_lists = [[3,4,5], [7,8,9]]\n",
    "\n",
    "# convert python list of lists to matrix\n",
    "np_arr = np.array(py_lists)\n",
    "print(\"np_arr=\\n\", np_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create numpy arrays using built in methods also\n",
    "\n",
    "# the following returns evenly spaced values in given interval\n",
    "# arange(start,stop,step)\n",
    "np.arange(0,11,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays with random numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the starting seed\n",
    "from numpy.random import randn\n",
    "np.random.seed(101)\n",
    "\n",
    "#use rand to create an array with random numbers from a uniform distribution\n",
    "np.random.rand(5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use randn to create an array with random numbers from a normal distribution with mean 0 and std deviation of 1\n",
    "np.random.randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use randint to create an array with random integers in a range from 1 to 100\n",
    "np.random.randint(1,100,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "![title](images/np-slicing.png)\n",
    "\n",
    "Image source: http://www.scipy-lectures.org/intro/numpy/numpy.html\n",
    "\n",
    "The general format is **arr_2d[row][col]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief overview Pandas\n",
    "\n",
    "### Series\n",
    "This is the basic data type for Pandas.\n",
    "\n",
    "A Series is built on top of the NumPy array object. The difference between a NumPy array and a Series is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also doesn't need to hold numeric data, it can hold any arbitrary Python Object.\n",
    "\n",
    "#### Creating a Series\n",
    "\n",
    "You can convert a list,numpy array, or dictionary to a Series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "labels = ['X','Y','Z']\n",
    "my_list = [10,20,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a series using a list\n",
    "pd.Series(data=my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a series using a numpy array\n",
    "arr = np.array([10,20,30])\n",
    "\n",
    "pd.Series(data=my_list,index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series using a dictionary\n",
    "D = {'a':50,'b':20,'c':30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using an Index\n",
    "\n",
    "Understanding how to use the index of the Series is key to using Pandas. Pandas makes use of these index names or numbers by allowing for fast look ups of information (works like a hash table or dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(['Asia','North America','Europe','Asia','Asia'],index = ['China','USA', 'Germany','USSR', 'Japan'])  \n",
    "\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser['USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "A DataFrame is the workhorse of pandas. We can think of a DataFrame as a bunch of Series objects put together to share the same index.\n",
    "\n",
    "![title](images/series-dataframe.png)\n",
    "\n",
    "Image source: https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import randn\n",
    "np.random.seed(99)\n",
    "\n",
    "df = pd.DataFrame(randn(4,3),index='B C D E'.split(),columns='X Y Z'.split())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print column X\n",
    "df['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a list of column names\n",
    "df[['X','Y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['X+Y'] = df['X'] + df['Y']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('X+Y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the drop command above is not \"in place\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can actually drop it as below\n",
    "df.drop('X+Y',axis=1,inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can select rows using the \"loc\" function\n",
    "df.loc['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you can select row using the \"iloc\" function\n",
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting subset of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??\n",
    "df.loc[['D','E'],['Y','Z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Information and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A':[1,2,np.nan,8],\n",
    "                  'B':[5,np.nan,np.nan,4],\n",
    "                  'C':[1,2,3,8],\n",
    "                  'D':[3,4,9,1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can drop missing data\n",
    "df.dropna()\n",
    "\n",
    "#NOTE that by default any row with missing data is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also drop columns with missing data (but we will not be using this much in this lecture)\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data and outputting data\n",
    "\n",
    "We will reading in csv files which are comma separated text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/RestaurantVisitors.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting data is as below\n",
    "df.to_csv('example.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime\n",
    "\n",
    "The datetime index in a pandas dataframe is based on numpy's datatime which is of type \"datetime64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CREATE AN ARRAY FROM THREE DATES\n",
    "np.array(['2016-03-15', '2017-05-24', '2018-08-09'], dtype='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that numpy has applied a day-level precision by default which gives us \"datetime64[D]\".\n",
    "\n",
    "We can specify the precision we want. For example to get yearly precision, we would specify as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dates = np.array(['2016-03-15', '2017-05-24', '2018-08-09'], dtype='datetime64[Y]')\n",
    "some_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll usually deal with time series as a datetime index when working with pandas dataframes. We can take the numpy array of dates that we created above and convert it into a pandas datetime index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to an index\n",
    "idx = pd.DatetimeIndex(some_dates)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets create a data frame with datetime index\n",
    "\n",
    "# Create some random data\n",
    "data = np.random.randn(3,2)\n",
    "cols = ['A','B']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with our random data, our date index, and our columns\n",
    "dframe = pd.DataFrame(data,idx,cols)\n",
    "dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='time-series-analysis'></a>\n",
    "# Time series Analysis\n",
    "Before getting to forecasting based on time series data we should understand how to analyze time data series to gain insights into what kind of models will fit this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code1'></a>\n",
    "### Coding Example: Read in a time series using pandas\n",
    "Let us look at this dataset from a <a href='https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting'>recent Kaggle competition</a> which considers daily visitors to four restaurants located in the United States, subject to American holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "##?? blank name of file\n",
    "df = pd.read_csv('./Data/RestaurantVisitors.csv',index_col='date',parse_dates=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='patterns'></a>\n",
    "## Patterns\n",
    "* **Trend**: long-term increase or decrease in the data. It does not have to be linear.\n",
    "* **Seasonal**: occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency.\n",
    "* **Cyclic**: occurs when the data exhibit rises and falls that are not of a fixed frequency. These rises and falls are many times related to say business cycles or economic conditions.\n",
    "\n",
    "Cyclic behaviour could be confused with seasonal behaviour-they are really quite different. If the fluctuations are not of a fixed frequency then they are cyclic; if the frequency is unchanging and associated with some aspect of the calendar, then the pattern is seasonal.\n",
    "\n",
    "Before deciding on a forecasting method, it is useful to look for patterns in the time series data.\n",
    "\n",
    "In the figure below, can you identify the patterns in each of the plots?\n",
    "\n",
    "![title](images/patterns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decompose'></a>\n",
    "## Decomposition of time series\n",
    "While looking for patterns in a time series in visual ways provides valuable insight, it is often difficult to see the patterns and using quantitive methods is preferred.\n",
    "\n",
    "<a id='ets'></a>\n",
    "### ETS Model for time series decomposition\n",
    "When we decompose a time series into components, we usually combine the trend and cycle into a single trend-cycle component. Then we have a seasonal component, and a remainder component (containing anything else in the time series) - so total of 3 components. We call this the ETS model or Error/Trend/Seasonality Model.\n",
    "\n",
    "Let $y_t$ represent the time series data. If we assume additive decomposition, then we can write $y_t = S_t + R_t + T_t$. Alternatively, multiplicative decomposition can be written as $y_t = S_t \\times R_t \\times T_t$. \n",
    "\n",
    "**Additive decomposition** is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. \n",
    "\n",
    "**Multiplicative decomposition** is more appropriate when the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series.\n",
    "\n",
    "Example of additive decomposition and multiplicative decomposition?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code2'></a>\n",
    "### Coding Example: Decompostion of Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first do the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data set for number of restaurant visitors\n",
    "#we set the index to the column which has the dates\n",
    "#we also tell pandas to identify the entries in the column as datetime\n",
    "df = pd.read_csv('./Data/RestaurantVisitors.csv',index_col='date',parse_dates=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rest1'].plot(figsize=(15,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(df['rest1'], model='additive') \n",
    "\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "#?\n",
    "result_mul = seasonal_decompose(df['rest1'], model='multiplicative') \n",
    "\n",
    "result_mul.plot();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.resid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mul.resid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='endo-exo'></a>\n",
    "## Endogenous versus Exogenous data\n",
    "* Endogenous: caused by factors within the system\n",
    "\n",
    "* Exogenous: caused by factors outside the system\n",
    "\n",
    "<a id='code3'></a>\n",
    "### Coding Example: identifying endogenous and exogenous data \n",
    "\n",
    "* Let us revisit the dataset that we just looked which considers daily visitors to four restaurants located in the United States, subject to American holidays.\n",
    "\n",
    "* The goal in the competition was to predict how many future visitors each of the 4 restaurants will receive\n",
    "\n",
    "* rest1, rest2, rest3 and rest4 are endogenous variables. Each of these can be predicted using past data.\n",
    "\n",
    "* holidays are exogenous variables as these are external factors that can affect how many visitors each of the restaurants can recieve on a given week day.\n",
    "\n",
    "\n",
    "**We will be working with endogenous variables in this lecture unless explicity stated as exogenous.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-fit'></a>\n",
    "## Model Fitting\n",
    "\n",
    "The time series example that we will use is the number of airline passengers on a monthly basis from 1941 to 1960.\n",
    "\n",
    "<a id='code4'></a>\n",
    "### Coding Example: Model fitting of time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first do the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#read in data set for number of airline passengers\n",
    "airline = pd.read_csv('./Data/airline_passengers.csv',index_col='Month',parse_dates=True)\n",
    "\n",
    "#drop any missing data\n",
    "airline.dropna(inplace=True)\n",
    "\n",
    "#plot\n",
    "airline.plot(figsize=(12,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that the frequency of data in the time series is monthly\n",
    "airline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sma'></a>\n",
    "### Simple Moving Average (SMA)\n",
    "This is the unweighted average of N previous samples i.e\n",
    "\n",
    "### $\\hat{y}_t =   \\frac{\\sum\\limits_{i=0}^{N-1} y_{t-i}}{N}$\n",
    "\n",
    "Pandas allows you to easily compute a SMA using \"rolling\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 month rolling mean\n",
    "airline.rolling(window=6).mean().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now plot this and an 18 month average with the original data\n",
    "\n",
    "## first we store the data as a column\n",
    "airline['6M-SMA'] = airline.rolling(window=6).mean()\n",
    "airline['18M-SMA'] = airline['Thousands of Passengers'].rolling(window=18).mean()\n",
    "\n",
    "##now plot\n",
    "airline.plot(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. Smaller windows will retain most of the variations and noise.\n",
    "2. Increasing the window size begins to show the trend but increases the lag.\n",
    "3. Peaks/valleys of the data are never reached due to the averaging.\n",
    "4. If window size is large, extreme historical values can skew the SMA significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ewma'></a>\n",
    "### Simple Exponentially Weighted Moving Average (EWMA)\n",
    "\n",
    "For the SMA method all observations in the window are given an equal weight. \n",
    "\n",
    "It may make more sense to attach larger weights to more recent observations than to observations from the distant past. This is the concept behind simple exponential smoothing. \n",
    "\n",
    "The formula for EWMA is:\n",
    "### $\\hat{y}_t =   \\frac{\\sum\\limits_{i=0}^t w_i y_{t-i}}{\\sum\\limits_{i=0}^t w_i}$\n",
    "\n",
    "\n",
    "where $y_t$ is the input value or the sample from the time series, $w_i$ is the applied weight and $\\hat{y}_t$ is the (estimated) output.\n",
    "\n",
    "There are various ways to specify the weight $w_i$ and the one we will try is:\n",
    " \\begin{split}w_i = \\begin{cases}\n",
    "    \\alpha (1 - \\alpha)^i & \\text{if } i < t \\\\\n",
    "    (1 - \\alpha)^i        & \\text{if } i = t.\n",
    "\\end{cases}\\end{split}\n",
    "\n",
    "which gives\n",
    "\n",
    "\n",
    "### $\\begin{split}\\hat{y}_0 &= y_0 \\\\\n",
    "\\hat{y}_t &= (1 - \\alpha) \\hat{y}_{t-1} + \\alpha y_t, \\\\\n",
    "&= (1- \\alpha) est_{prev} + (\\alpha) obs_{cur} \\end{split}$ \n",
    "\n",
    "Writing out the estimate in terms of the time series data:\n",
    "$\\begin{split}\n",
    "\\hat{y}_1 &= \\alpha y_1 + (1 - \\alpha) y_0 , \\\\ \n",
    "\\hat{y}_2 &=  \\alpha y_2 + (1-\\alpha) \\alpha y_1 + (1 - \\alpha)^2 y_0  \n",
    "\\end{split}$\n",
    "\n",
    "<a id='time-constant'></a>\n",
    "### Time constant\n",
    "The time constant of an EWMA is the amount of time for the smoothed response to a unit step function to reach $1-\\frac{1}{e} = 63.2%$ of the original signal. Based on this, the time constant can be written as:\n",
    "\n",
    "$\\tau \\approx \\frac{\\Delta T}{\\alpha} $ where $\\Delta T$ is sampling time interval of the data series.\n",
    "\n",
    "In our example, $\\Delta T = 1$ month and so $\\tau \\approx \\frac{1}{\\alpha}$ months.\n",
    "\n",
    "|$\\alpha$|$\\tau$      |  \n",
    "|--------|------------| \n",
    "|0.8     | 1.25 months| \n",
    "|0.2     | 5 months   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the \"ewm\" that pandas provides\n",
    "airline['EWMA-alpha0.8'] = airline['Thousands of Passengers'].ewm(alpha=0.8,adjust=False).mean()\n",
    "\n",
    "# try alpha=0.2\n",
    "airline['EWMA-alpha0.2'] = airline['Thousands of Passengers'].ewm(alpha=0.2,adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline[['Thousands of Passengers','EWMA-alpha0.8','EWMA-alpha0.2']].plot(figsize=(14,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='time-series-forecasting'></a>\n",
    "# Time series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Average method\n",
    "The forecasts of all future values are equal to the average (or “mean”) of the historical data. If we let the historical data be denoted by $y_1$,...,y_T$ then the forecasts can be written as\n",
    "\n",
    "### $\\hat{y}_{{T+h}|T} = \\frac{\\sum\\limits_{i=0}^{T-1} y_i }{T}$\n",
    "\n",
    "\n",
    "### Naive Method\n",
    "We simply set all forecasts to be the value of the last observation, i.e.\n",
    "### $\\hat{y}_{{T+h}/T} = y_T$\n",
    "\n",
    "### Seasonal Naive Method\n",
    "This is for seasonal data where we set each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year).\n",
    "\n",
    "### $\\hat{y}_{{T+h}/T} = y_{{T+h}-m(k+1)}$\n",
    "where $k$ is the integer part of $\\frac{h-1}{M}$\n",
    "\n",
    "![title](images/simple-forecast.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Smoothing Method\n",
    "Earlier we had seen that for exponential smoothing:\n",
    "\n",
    "### $\\begin{split}\\hat{y}_0 &= y_0 \\\\\n",
    "\\hat{y}_t &= (1 - \\alpha) \\hat{y}_{t-1} + \\alpha y_t \\end{split}$\n",
    "\n",
    "When we talk of forecasting, there is what is called the \"component form\" where we have a smoothing equation and a forecasting equation:\n",
    "\n",
    "**Smoothing equation**: $l_t = (1 - \\alpha) l_{t-1} + \\alpha y_t $\n",
    "\n",
    "**Forecasting equation/model**:  $\\hat{y}_{t+1} =  l_t$ \n",
    "\n",
    " \n",
    "Note that with exponential smoothing we are primarily tracking the \"level\" and we have only one smoothing parameter $\\alpha$.\n",
    "\n",
    "One could have double exponential smoothing and triple exponential smoothing via **Holt Winters Method**.\n",
    "\n",
    "## Holt-Winters Method\n",
    "\n",
    "The Holt-Winters method is used to perform double and triple exponential smoothing.\n",
    "\n",
    "In <strong>Double Exponential Smoothing</strong> we introduce a new smoothing factor $\\beta$ (beta) that accounts for **trend**:\n",
    "\n",
    "\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n",
    "b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n",
    "y_t &= l_t + b_t & \\text{    fitted model}\\\\\n",
    "\\hat y_{t+h} &= l_t + hb_t & \\text{    forecasting model (} h = \\text{# periods into the future)}\\end{split}\n",
    "\n",
    "\n",
    "\n",
    "With <strong>Triple Exponential Smoothing</strong> we introduce a smoothing factor $\\gamma$ (gamma) that accounts for **seasonality**:\n",
    "\n",
    "\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n",
    "b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n",
    "c_t &= (1-\\gamma)c_{t-L} + \\gamma(x_t-l_{t-1}-b_{t-1}) & \\text{    seasonal}\\\\\n",
    "y_t &= (l_t + b_t) c_t & \\text{    fitted model}\\\\\n",
    "\\hat y_{t+m} &= (l_t + mb_t)c_{t-L+1+(m-1)modL} & \\text{    forecasting model (} m = \\text{# periods into the future)}\\end{split}\n",
    "\n",
    "Here $L$ represents the number of divisions per cycle. For example, if we are looking at monthly data that displays a repeating pattern each year, we would use $L=12$.\n",
    "\n",
    "In general, higher values for $\\alpha$, $\\beta$ and $\\gamma$ (values closer to 1), place more emphasis on recent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Example: Forecasting using Holt Winters Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue to use the airline passenger data series\n",
    "\n",
    "# We do the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#READ IN the dataset file\n",
    "air = pd.read_csv('./Data/airline_passengers.csv',index_col='Month',parse_dates=True)\n",
    "\n",
    "#quickly look at the data\n",
    "air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLEAN THE DATA\n",
    "#drop any missing values\n",
    "air.dropna(inplace=True)\n",
    "\n",
    "#let us look at the index\n",
    "air.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatetimeIndex Frequency\n",
    "Notice that the \"freq\" is None. To use Holt-Winters smoothing model, statsmodels needs to know the frequency of the data (whether it's daily, monthly etc.). In our case it is monthly and so we'll use MS. <br>A full list of time series offset options can be found at <a href='http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air.index.freq = 'MS'\n",
    "air.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation steps for forecasting\n",
    "\n",
    "![title](images/forecast-steps-implement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT DATA INTO TRAIN AND TEST\n",
    "#let us first see how much data we have\n",
    "air.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generally rough 80/20 split for train versus test is resonable\n",
    "#Alternatively you might want the test size to be comparable to the forecast size\n",
    "# in our case about 24 test and 120 train\n",
    "train_data = air.iloc[:120] # Goes up to but not including 120\n",
    "test_data = air.iloc[120:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CHOOSE THE MODEL\n",
    "# let us take a look at the plot of the data again and look for patterns\n",
    "air.plot(figsize=(12,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe above that we have a trend (non-linear) and seasonal (increasing) component and so we should choose our model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "#CHOOSE AND FIT MODEL\n",
    "# we will first try simple exponential smoothing\n",
    "# due to non-linear rise in trend, we will specify trend to be multiplicative\n",
    "# due to increase seasonal variation with time, we will specify seasonal component also to be multiplicative\n",
    "# we also specify that the seasonal cycle here is 12 months\n",
    "fitted_model = ExponentialSmoothing(train_data['Thousands of Passengers'],trend='mul',seasonal='mul',seasonal_periods=12).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now want to predict for the test data. We need to know how many points does the test data have\n",
    "len_test = len(test_data)\n",
    "test_predictions = fitted_model.forecast(len_test).rename('Test Forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PLOT to compare predicted test data with ground truth\n",
    "train_data['Thousands of Passengers'].plot(legend=True,label='TRAIN')\n",
    "test_data['Thousands of Passengers'].plot(legend=True,label='TEST',figsize=(12,8))\n",
    "test_predictions.plot(legend=True,label='TEST-PREDICTED');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics\n",
    "A forecast “error” is the difference between an observed value and its forecast and can be written as:\n",
    "$e_t = y_t - \\hat{y}_t$\n",
    "\n",
    "The forecasting errors are computed on the test data.\n",
    "\n",
    "Two popular evaluation metrics used are:\n",
    "\n",
    "Mean Absolute Error (MAE) = mean($|e_t|$) (forecast of the median)\n",
    "\n",
    "Root Mean Square Error (RMSE) = $\\sqrt{mean(e_t^2)}$ (forecast of the mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "mean_absolute_error(test_data,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(test_data,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare to mean to get a sense how big the error is\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice the difference between the forecasting model and fitted model. Below we are fitting on all data whereas for the fitted\n",
    "#model we are fitting on the training data\n",
    "forecast_model = ExponentialSmoothing(air['Thousands of Passengers'],trend='mul',seasonal='mul',seasonal_periods=12).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us forecast 48 months into future\n",
    "forecast_predictions = forecast_model.forecast(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air['Thousands of Passengers'].plot(figsize=(12,8))\n",
    "forecast_predictions.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Models\n",
    "\n",
    "### (Auto) Regression\n",
    "\n",
    "The basic concept of regression in general terms is that we forecast the time series of interest $y$ assuming that it has a linear relationship with other time series $x$. \n",
    "\n",
    "$y$ is called the **forecast variable** or regressand, dependent or explained variable.\n",
    "\n",
    "$x$ is called the **predictor variable** or regressor, independent or explanatory variable. \n",
    "\n",
    "**Autoregression**: we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself.\n",
    "\n",
    "### Backshifting or Lagging\n",
    "Backshifting or lagging notation reflects the value of $y$ at a prior point in time. This is a useful technique for performing <em>regressions</em>.\n",
    "\n",
    "\\begin{split}L{y_t} = y_{t-1} & \\text{      one lag shifts the data back one period}\\\\\n",
    "L^{2}{y_t} = y_{t-2} & \\text{      two lags shift the data back two periods} \\\\ \n",
    "L^{3}{y_t} = y_{t-3} & \\text{      three lags shift the data back three periods} \\end{split}\n",
    "<br><br>\n",
    "<table>\n",
    "<tr><td>$y_t$</td><td>6</td><td>8</td><td>3</td><td>4</td><td>9</td><td>2</td><td>5</td></tr>\n",
    "<tr><td>$y_{t-1}$</td><td>8</td><td>3</td><td>4</td><td>9</td><td>2</td><td>5</td></tr>\n",
    "<tr><td>$y_{t-2}$</td><td>3</td><td>4</td><td>9</td><td>2</td><td>5</td></tr>\n",
    "<tr><td>$y_{t-3}$</td><td>4</td><td>9</td><td>2</td><td>5</td></tr>\n",
    "</table>\n",
    "\n",
    "### Stationarity\n",
    "Time series data is said to be <em>stationary</em> if its properties do not depend on the time at which the series is observed i.e. it does <em>not</em> exhibit trends or seasonality.  On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time.\n",
    "\n",
    "### Differencing\n",
    "Non-stationary data can be made to look stationary through differencing. A simple differencing method calculates the difference between consecutive points.\n",
    "\n",
    "### Example: Can you classify each of the time series below as stationary or non-stationary?\n",
    "![title](images/stat-diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of ARIMA\n",
    "\n",
    "Auto Regression + Integration + Moving Average = ARIMA\n",
    "\n",
    "<strong>ARIMA</strong>, or <em>Autoregressive Integrated Moving Average</em> is actually a combination of 3 models:\n",
    "* <strong>AR(p)</strong> Autoregression - a regression model that utilizes the dependent relationship between a current observation and observations over a previous period\n",
    "* <strong>I(d)</strong> Integration - uses differencing of observations (subtracting an observation from an observation at the previous time step) in order to make the time series stationary\n",
    "* <strong>MA(q)</strong> Moving Average - a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
    "\n",
    "<strong>Moving Averages</strong> e.g.  EWMA and the Holt-Winters Method.<br>\n",
    "<strong>Integration</strong> use differencing to make a time series stationary, which ARIMA requires.<br>\n",
    "<strong>Autoregression</strong> as already discussed, we correlate a current time series with a lagged version of the same series.<br>\n",
    "We will need to choose the $p$, $d$ and $q$ values required by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Model or AR(p)\n",
    "The previous models that we have looked were some form of moving averages models where we forecast the variable of interest using a linear combination of predictors. In the \"airline passengers example\" we forecasted the numbers of airline passengers based on a set of level, trend and seasonal predictors.\n",
    "\n",
    "As explained previously with an autoregression model, we forecast using a linear combination of <em>past values</em> of the variable. An autoregression is run against a set of <em>lagged values</em> of order $p$.\n",
    "\n",
    "**We usually restrict autoregressive models to stationary data.**\n",
    "\n",
    "### $y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t}$\n",
    "\n",
    "where $c$ is a constant, $\\phi_{1}$ and $\\phi_{2}$ are lag coefficients up to order $p$, and $\\varepsilon_{t}$ is white noise.\n",
    "\n",
    "For example, an <strong>AR(1)</strong> model would follow the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "whereas an <strong>AR(2)</strong> model would follow the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\varepsilon_{t}$\n",
    "\n",
    "and so on.\n",
    "\n",
    "Note that the lag coefficients are usually less than one.<br>\n",
    "Specifically, for an <strong>AR(1)</strong> model: $-1 \\lt \\phi_1 \\lt 1$<br>\n",
    "and for an <strong>AR(2)</strong> model: $-1 \\lt \\phi_2 \\lt 1, \\ \\phi_1 + \\phi_2 \\lt 1, \\ \\phi_2 - \\phi_1 \\lt 1$<br>\n",
    "\n",
    "Models <strong>AR(3)</strong> and higher become mathematically very complex. Fortunately statsmodels does all the heavy lifting for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Moving Averages - ARMA(p,q)\n",
    "\n",
    "\n",
    "\n",
    "Recall that an <strong>AR(1)</strong> model follows the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "while an <strong>MA(1)</strong> model follows the formula\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = \\mu + \\theta_{1}\\varepsilon_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "where $c$ is a constant, $\\mu$ is the expectation of $y_{t}$ (often assumed to be zero), $\\phi_1$ (phi-sub-one) is the AR lag coefficient, $\\theta_1$ (theta-sub-one) is the MA lag coefficient, and $\\varepsilon$ (epsilon) is white noise.\n",
    "\n",
    "An <strong>ARMA(1,1)</strong> model therefore follows\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$y_{t} = c + \\phi_{1}y_{t-1} + \\theta_{1}\\varepsilon_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "**ARMA models can be used on stationary datasets.**\n",
    "\n",
    "**For non-stationary datasets with a trend component, ARIMA models apply a differencing coefficient as well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Example of forecasting with ARMA\n",
    "We will use a stationary dataset which is not easy to find but the first four months of the <em>Daily Total Female Births</em> dataset can be considered to be stationary.\n",
    "\n",
    "The goal is to determine (p,q) orders and run a forecasting ARMA model fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Load specific forecasting tools\n",
    "#from statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults\n",
    "from statsmodels.tsa.arima_model import ARMA,ARMAResults\n",
    "#from statsmodels.graphics.tsaplots import plot_acf,plot_pacf # for determining (p,q) orders\n",
    "from pmdarima import auto_arima # for determining ARIMA orders\n",
    "\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv('./Data/DailyTotalFemaleBirths.csv',index_col='Date',parse_dates=True)\n",
    "df1.index.freq = 'D'\n",
    "df1 = df1[:120]  # we only want the first four months\n",
    "\n",
    "#let us plot to see how this data looks\n",
    "df1['Births'].plot(figsize=(12,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pmdarima Auto-ARIMA\n",
    "**pmdarima** is a third-party tool separate from statsmodels. Let us take a quick look at this tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us import the auto_arima from pmdarima\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#let us take a quick look at the help\n",
    "help(auto_arima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the (p,q) ARMA Orders using <tt>pmdarima.auto_arima</tt>\n",
    "\n",
    "\n",
    "\n",
    "This tool should give just $p$ and $q$ value recommendations for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we set the seasonal component to false since there is no seasonal component in this data\n",
    "auto_arima(df1['Births'],seasonal=False).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/test sets\n",
    "In general it is a good idea to set the length of your test set equal to your intended forecast size. For this dataset we'll attempt a 1-month forecast.\n",
    "\n",
    "Recall that we have 4 months of data. So we will use the first 3 months or 90 days of data for training and the remaining 1 month or 30 days of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set one month for testing\n",
    "train = df1.iloc[:90]\n",
    "test = df1.iloc[90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit an ARMA(p,q) Model\n",
    "If you want you can run <tt>help(ARMA)</tt> to learn what incoming arguments are available/expected, and what's being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARMA(train['Births'],order=(2,2))\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a month's worth of predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=len(train)\n",
    "end=len(train)+len(test)-1\n",
    "predictions = results.predict(start=start, end=end).rename('ARMA(2,2) Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predictions against known values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Daily Total Female Births'\n",
    "ylabel='Births'\n",
    "xlabel='' # we don't really need a label here\n",
    "\n",
    "ax = test['Births'].plot(legend=True,figsize=(12,6),title=title)\n",
    "predictions.plot(legend=True)\n",
    "ax.autoscale(axis='x',tight=True)\n",
    "ax.set(xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model = ARMA(df1['Births'],order=(2,2))\n",
    "results = forecast_model.fit()\n",
    "forecast_predictions = results.predict(len(df1),len(df1)+30).rename('ARMA(2,2) Forecast')\n",
    "\n",
    "df1['Births'].plot(figsize=(12,5),legend=True)\n",
    "predictions.plot(legend=True)\n",
    "forecast_predictions.plot(legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our starting dataset exhibited no trend or seasonal component, this prediction makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISCELLANEOUS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Correlation Function (ACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "df = pd.read_csv('./Data/RestaurantVisitors.csv',index_col='date',parse_dates=True)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "result_mul = seasonal_decompose(df['rest1'], model='multiplicative') \n",
    "result_mul.resid = result_mul.resid.dropna()\n",
    "#result_mul.resid.plot(figsize=(15,5))\n",
    "print(acf(result_mul.resid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
